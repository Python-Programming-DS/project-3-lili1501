{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROJECT 3 — MACHINE LEARNING MODELING REPORT\n",
    "## Part A: Logistic Regression\n",
    "## Part B: Random Forest Claasifier (Non-Logistic Model)\n",
    "\n",
    "### 1. Introduction\n",
    "The objective of this project is to build and evaluate predictive models using a cleaned and pre processed dataset.The modeling process is divided into two stages: Logistic Regression (Part A) where I used feature engineering to try to  improve the ROC-AUC score as well as the accuracy and the F1 score and a more advanced non-logistic model (Random Forest, Part B).\n",
    "\n",
    "### 2. Data Cleaning & Preprocessing\n",
    "Key steps included:\n",
    "- Removing unwanted characters like ## % _ etc \n",
    "- Dropping irrelevant columns which were either empty or represented just as column ids which did not give any useful information.\n",
    "- Handled missing data (either nan or inf) with the median for the numeric columns and mode for categorical columns.\n",
    "- Dummy (pd.get_dummies) encoding categorical variables for object cols identified from .info. As well as converted the numeric looking cols to numeric type.\n",
    "- Removed highly correlated columns over 0.8\n",
    "- Created quantile bins after identifying continuous variable cols by checking no.of unique values in each col.\n",
    "- Created few visulation to check the correlation of the clean df, used the heatmap to visualize the null values, analysis for continuous variable in the col, box plot to detect outliers.\n",
    "- Saved the clean data as csv file in the name : \"cleaned_new_train_data.csv\"\n",
    "\n",
    "### 3. Part A — Logistic Regression Model\n",
    "- logistic regression model showed stable performance with or without hyperparameter tuning. The validation accuracy, F1-score, and AUC remained essentially unchanged before and after applying RandomizedSearchCV\n",
    "This indicates that:\n",
    "The dataset is well suited for logistic regression\n",
    "The model is not highly sensitive to the choice of regularization strength (C)\n",
    "The default logistic regression configuration already achieves near-optimal performance\n",
    "Hyperparameter tuning primarily confirmed the best solver/penalty rather than improving predictive power.\n",
    "\n",
    "- For continuous variables I have used standard scaler for those columns.\n",
    "\n",
    "**Best Parameters:** {'solver': 'liblinear', 'penalty': 'l1', 'C': 0.078}\n",
    "\n",
    "**Validation Metrics:**\n",
    "- Accuracy: 0.878\\n\n",
    "- Precision: 0.869\\n\n",
    "- Recall: 0.847\\n\n",
    "- F1: 0.858\\n\n",
    "- AUC: 0.95\\n\n",
    "\n",
    "**Test Metrics:** \\n\n",
    "- Accuracy: 0.872\\n\n",
    "- Precision: 0.866\\n\n",
    "- Recall: 0.833\\n\n",
    "- F1: 0.849\\n\n",
    "- AUC: 0.94\\n\n",
    "\n",
    "### Top Logistic Regression Coefficients to identify interactive features\n",
    "- Positive: x25_LC, x14, x17, x20, x18, x9\n",
    "- Negative: x26_PT, x4, x28\n",
    "\n",
    "-In addition to standard preprocessing and scaling, I explored interaction features to help logistic regression capture relationships that are not purely linear. Logistic regression by itself assumes additive effects, meaning each feature contributes independently to the log-odds. However, in many real-world datasets, the effect of one variable may depend on the value of another variable.\n",
    "\n",
    "To capture such dependencies, I created a set of pairwise interaction terms between the continuous variables:\n",
    "\n",
    "x23_x29 = x23 * x29\n",
    "\n",
    "x23_x31 = x23 * x31\n",
    "\n",
    "x29_x31 = x29 * x31\n",
    "\n",
    "x5_x23 = x5 * x23\n",
    "\n",
    "x5_x29 = x5 * x29\n",
    "\n",
    "These products allow the logistic model to understand patterns like:\n",
    "\n",
    "“When x23 is high and x29 is high, the risk increases more sharply.”\n",
    "\n",
    "“The effect of x5 may only be visible when combined with x23.”\n",
    "\n",
    "*** Feature engineering (interaction + log transforms for Part A)\n",
    "\n",
    "**Validation Metrics:**\n",
    "- Accuracy: 0.880\\n\n",
    "- Precision: 0.873\\n\n",
    "- Recall: 0.848\\n\n",
    "- F1: 0.860\\n\n",
    "- AUC: 0.95\\n\n",
    "\n",
    "**Test Metrics:** \\n\n",
    "- Accuracy: 0.873\\n\n",
    "- Precision: 0.866\\n\n",
    "- Recall: 0.833\\n\n",
    "- F1: 0.850\\n\n",
    "- AUC: 0.94\\n\n",
    "\n",
    "- It can be seen the validation metrics like accuracy and F1 slightly improved while for test metrics it only improved by 0.001.The ROC-AUC didn't had any significant changes.\n",
    "\n",
    "### 4. Part B — Random Forest Classifier\n",
    "Here I decided to go with random forest classifier, hyperparameter using randomsearchedCV.\n",
    "**Best Parameters:** class_weight='balanced', max_features=0.7, min_samples_leaf=2, min_samples_split=10, n_estimators=400\n",
    "\n",
    "**Validation Metrics:**\\n\n",
    "- Accuracy: 0.964\\n\n",
    "- Precision: 0.970\\n\n",
    "- Recall: 0.946\\n\n",
    "- F1: 0.958\\n\n",
    "- AUC: 0.99\\n\n",
    "\n",
    "**Test Metrics:**\\n\n",
    "- Accuracy: 0.962\\n\n",
    "- Precision: 0.969\\n\n",
    "- Recall: 0.942\\n\n",
    "- F1: 0.955\\n\n",
    "- AUC: 0.99\\n\n",
    "\n",
    "### Top Random Forest Features\n",
    "- x14, x9, x26_PT, x28, x16, x25_LC, x5, x20, x18, x29\n",
    "\n",
    "The Random Forest model provides feature importance values that indicate how frequently and how effectively each variable is used to split the data into more homogeneous groups. Higher importance means the feature plays a stronger role in determining the prediction. Based on the tuned Random Forest model, the top contributing features were:\n",
    "\n",
    "1. x14 (Most Important Feature)\n",
    "\n",
    "x14 was consistently the strongest predictor of the target.\n",
    "Its high importance indicates that this variable frequently helped differentiate between the two classes through clean and effective split points. This suggests that x14 captures a strong underlying pattern or threshold effect associated with the outcome.\n",
    "\n",
    "2. x9\n",
    "\n",
    "x9 also ranked highly and likely interacts with other predictors in determining the risk.\n",
    "Random Forest models naturally account for such interactions, which may explain why this feature gained more importance compared to its coefficient weight in logistic regression.\n",
    "\n",
    "3. x26_PT\n",
    "\n",
    "This categorical indicator (_PT) had strong discriminative power.\n",
    "Its importance means that belonging to the PT category has a notable impact on the predicted class. Interestingly, this feature had one of the most negative coefficients in logistic regression, and the Random Forest’s high importance further validates its relevance.\n",
    "\n",
    "4. x28\n",
    "\n",
    "x28 emerged as another influential feature.\n",
    "Its importance shows it may represent a non-linear or threshold-driven relationship, which tree-based models capture more effectively than logistic regression.\n",
    "\n",
    "5. x16\n",
    "\n",
    "Though not extremely strong in the logistic regression model, x16 appears frequently in Random Forest splits, suggesting it interacts with other predictors or has non-linear boundaries.\n",
    "\n",
    "6. x25_LC\n",
    "\n",
    "This categorical variable (_LC) had one of the highest positive coefficients in logistic regression and also ranks among top features in the tree model.\n",
    "Both models agree on its strong association with the target.\n",
    "\n",
    "7. x5\n",
    "\n",
    "x5 is one of the continuous features.\n",
    "Its importance indicates that the Random Forest found useful thresholds in the variable, showing non-linear effects (e.g., risk increasing or decreasing rapidly past certain values).\n",
    "\n",
    "8. x20 & x18\n",
    "\n",
    "Both features appear moderately important.\n",
    "Their ranking suggests they do not individually drive predictions but likely interact with other variables to improve split quality.\n",
    "\n",
    "9. x29\n",
    "\n",
    "As another continuous variable, x29 contributes meaningfully to model predictions.\n",
    "Although its effect appears subtle in logistic regression, the Random Forest reveals more complex patterns involving this variable.\n",
    "\n",
    "### 5. Model Comparison\n",
    "\n",
    "This project evaluated two modeling approaches:\n",
    "- Logistic Regression (Part A) – a linear, interpretable model\n",
    "- Random Forest (Part B) – a nonlinear, tree-based ensemble model\n",
    "Both models were trained on the same cleaned dataset and evaluated on validation and test sets.\n",
    "\n",
    "Performance Summary:\n",
    "| Metric | Logistic Regression | Random Forest |\n",
    "|--------|--------------------|---------------|\n",
    "| Test AUC | 0.94 | 0.99 |\n",
    "| Test Accuracy | 0.873 | 0.962 |\n",
    "| Test F1 | 0.850 | 0.955 |\n",
    "\n",
    "Random Forest performs substantially better than Logistic Regression across every metric.\n",
    "The improvement is especially large in Recall, F1 Score, and overall accuracy.\n",
    "\n",
    "Random Forest performs better\n",
    "\n",
    "1. Captures Nonlinear Relationships\n",
    "\n",
    "Logistic Regression assumes a linear relationship between each feature and the outcome.\n",
    "Random Forest does not—it automatically learns:\n",
    "\n",
    "nonlinear trends\n",
    "\n",
    "thresholds\n",
    "\n",
    "interactions between features\n",
    "\n",
    "This gives it a major advantage on complex datasets.\n",
    "\n",
    "2. Handles Feature Interactions Automatically\n",
    "\n",
    "In Part A, interaction terms had to be manually engineered and tested.\n",
    "In Part B, Random Forest learns interactions on its own through tree splits.\n",
    "\n",
    "3. More Robust to Irrelevant or Noisy Features\n",
    "\n",
    "Logistic Regression is sensitive to multicollinearity and scaling.\n",
    "Random Forest is not—it naturally selects informative features.\n",
    "\n",
    "4. Consistent Generalization\n",
    "\n",
    "Even after hyperparameter tuning, Random Forest achieved:\n",
    "\n",
    "Train AUC: 0.9996\n",
    "\n",
    "Validation/Test AUC: 0.99\n",
    "\n",
    "The small gap indicates excellent generalization with minimal overfitting.\n",
    "\n",
    "### 6. Conclusion\n",
    "Random Forest significantly outperforms Logistic Regression on all metrics, showing excellent generalization and robustness after hyperparameter tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
